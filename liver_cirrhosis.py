# -*- coding: utf-8 -*-
"""liver_cirrhosis.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1XIFaR9fcKdtoHknkvqME_Wmim5rx99Ma
"""

# 1. Imports
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold
from sklearn.preprocessing import LabelEncoder, StandardScaler, RobustScaler, PowerTransformer
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, VotingClassifier
from sklearn.svm import SVC
from xgboost import XGBClassifier
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, roc_curve, auc, precision_recall_curve
from sklearn.feature_selection import SelectFromModel, RFE
from sklearn.pipeline import Pipeline
from sklearn.decomposition import PCA
from scipy import stats
import warnings
warnings.filterwarnings('ignore')

# 2. Load dataset with error handling
try:
    df = pd.read_csv('liver_cirrhosis.csv')
    print(f"Dataset loaded successfully with {df.shape[0]} rows and {df.shape[1]} columns")
except FileNotFoundError:
    print("File not found. Please check the path.")
    # Create dummy data for demonstration
    df = pd.DataFrame(np.random.random((100, 10)))

# 3. Advanced preprocessing
# 3.1 Missing value handling with more sophisticated approach
print("Missing values before treatment:", df.isnull().sum().sum())
df_missing_stats = pd.DataFrame({
    'Missing_Count': df.isnull().sum(),
    'Missing_Percent': (df.isnull().sum() / len(df)) * 100
})
print("\nMissing value statistics:")
print(df_missing_stats[df_missing_stats['Missing_Count'] > 0])

# Drop rows with missing values, but only if missing values are less than 5%
threshold = 0.05 * len(df)
cols_to_drop = df_missing_stats[df_missing_stats['Missing_Count'] > threshold].index.tolist()
if cols_to_drop:
    print(f"Dropping columns with too many missing values: {cols_to_drop}")
    df.drop(cols_to_drop, axis=1, inplace=True)

df.dropna(inplace=True)  # Drop remaining rows with missing values
print("Missing values after treatment:", df.isnull().sum().sum())

# 3.2 Enhanced encoding of categorical variables
cat_cols = ['Status', 'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']
encoders = {}

# Save original mappings for later interpretation
cat_mappings = {}
for col in cat_cols:
    if col in df.columns:
        le = LabelEncoder()
        df[col] = le.fit_transform(df[col])
        encoders[col] = le
        cat_mappings[col] = dict(zip(le.classes_, le.transform(le.classes_)))
        print(f"Encoding for {col}: {cat_mappings[col]}")

# 4. Enhanced EDA
# 4.1 Outlier detection
def detect_outliers(df, n_std=3):
    """Detect outliers using z-score method"""
    outliers = {}
    numeric_cols = df.select_dtypes(include=[np.number]).columns

    for col in numeric_cols:
        # Calculate z-scores
        z_scores = np.abs(stats.zscore(df[col].dropna()))
        # Find outliers
        outliers[col] = (df[col].dropna().iloc[np.where(z_scores > n_std)]).values

    return outliers

outliers = detect_outliers(df)
print("\nOutlier Analysis:")
for col, values in outliers.items():
    if len(values) > 0:
        print(f"{col}: {len(values)} outliers detected")

# 4.2 Distribution analysis
numeric_cols = df.select_dtypes(include=[np.number]).columns
skewness = df[numeric_cols].skew()
print("\nSkewness Analysis:")
print(skewness[abs(skewness) > 0.5])

# Create visualization function for later use
def create_visualizations(df, target_col):
    """Create visualizations for EDA"""
    # Correlation heatmap
    plt.figure(figsize=(12, 10))
    corr = df.corr()
    mask = np.triu(np.ones_like(corr, dtype=bool))
    sns.heatmap(corr, mask=mask, annot=False, cmap='coolwarm', linewidths=0.5)
    plt.title("Correlation Matrix")
    plt.tight_layout()

    # Feature distributions by target class
    if target_col in df.columns:
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        numeric_cols = [col for col in numeric_cols if col != target_col]

        for i, col in enumerate(numeric_cols[:5]):  # Plot first 5 numeric features
            plt.figure(figsize=(10, 6))
            sns.histplot(data=df, x=col, hue=target_col, kde=True, element="step")
            plt.title(f"Distribution of {col} by {target_col}")

    # Distribution of target variable
    if target_col in df.columns:
        plt.figure(figsize=(8, 6))
        sns.countplot(data=df, x=target_col)
        plt.title(f"Distribution of {target_col}")

    return "Visualizations created (uncomment to display)"

# Uncomment to display EDA visualizations
# viz_result = create_visualizations(df, 'Stage')
# plt.show()

# 5. Advanced feature engineering and selection
# 5.1 Define features and target with feature engineering
if 'Stage' in df.columns:
    # Print target variable information before any transformation
    print("\nTarget variable (Stage) unique values before transformation:", df['Stage'].unique())
    print("Target variable value counts:", df['Stage'].value_counts())

    # FIX: Transform Stage values to start from 0 (subtract 1 if they start from 1)
    if df['Stage'].min() > 0:
        print(f"Transforming Stage values from {df['Stage'].unique()} to start from 0")
        df['Stage'] = df['Stage'] - df['Stage'].min()
        print("Target variable unique values after transformation:", df['Stage'].unique())

    # Feature Engineering: Create interaction terms
    numeric_cols = df.select_dtypes(include=[np.number]).columns
    numeric_cols = [col for col in numeric_cols if col != 'Stage']

    # Create interaction terms for key features (choose a few important ones)
    important_features = numeric_cols[:3]  # Choose first 3 numeric features for interaction
    for i, feat1 in enumerate(important_features):
        for feat2 in important_features[i+1:]:
            df[f'{feat1}_x_{feat2}'] = df[feat1] * df[feat2]

    # Create polynomial features for key metrics
    for feat in important_features:
        df[f'{feat}_squared'] = df[feat] ** 2

    # Log transform skewed features
    skewed_features = skewness[abs(skewness) > 1].index
    for feat in skewed_features:
        if feat in df.columns and feat != 'Stage':
            # Add small constant to handle zeros
            df[f'{feat}_log'] = np.log1p(df[feat] - df[feat].min() + 0.01)

    # Define X and y
    X = df.drop(['Stage'], axis=1)
    y = df['Stage']

    # Advanced feature selection using Random Forest
    print("\nPerforming feature selection...")
    feat_selector = SelectFromModel(RandomForestClassifier(n_estimators=100, random_state=42))
    feat_selector.fit(X, y)

    selected_features = X.columns[feat_selector.get_support()]
    print(f"Selected {len(selected_features)} features: {selected_features.tolist()}")

    X_selected = feat_selector.transform(X)
    print(f"X shape after feature selection: {X_selected.shape}")

    # 5.2 Feature scaling with options
    # Create a pipeline with different scaling options
    scalers = {
        'standard': StandardScaler(),
        'robust': RobustScaler(),
        'power': PowerTransformer(method='yeo-johnson')
    }

    # Choose a scaler (standard for now, but we could make this dynamic)
    chosen_scaler = 'standard'
    X_scaled = scalers[chosen_scaler].fit_transform(X_selected)

    # 5.3 Dimensionality reduction (optional)
    apply_pca = False  # Set to True to apply PCA
    if apply_pca:
        pca = PCA(n_components=0.95)  # Retain 95% of variance
        X_scaled = pca.fit_transform(X_scaled)
        print(f"PCA reduced dimensions to {X_scaled.shape[1]} components")

    # 6. Enhanced train/test split with stratification
    X_train, X_test, y_train, y_test = train_test_split(
        X_scaled, y, test_size=0.2, random_state=42, stratify=y
    )
    print(f"\nTrain set shape: {X_train.shape}, Test set shape: {X_test.shape}")
    print(f"Target distribution in train set: {np.bincount(y_train)}")
    print(f"Target distribution in test set: {np.bincount(y_test)}")

    # 7. Model training with hyperparameter tuning and cross-validation
    # 7.1 Define models with hyperparameters
    models = {
        'RandomForest': {
            'model': RandomForestClassifier(random_state=42),
            'params': {
                'n_estimators': [100, 200],
                'max_depth': [None, 10, 20],
                'min_samples_split': [2, 5]
            }
        },
        'SVM': {
            'model': SVC(kernel='rbf', probability=True, random_state=42),
            'params': {
                'C': [0.1, 1, 10],
                'gamma': ['scale', 'auto', 0.1]
            }
        },
        'XGBoost': {
            'model': XGBClassifier(
                use_label_encoder=False,
                eval_metric='mlogloss',
                random_state=42,
                # FIX: Explicitly set objective for classification
                objective='multi:softprob'
            ),
            'params': {
                'n_estimators': [100, 200],
                'learning_rate': [0.01, 0.1],
                'max_depth': [3, 5, 7]
            }
        },
        'GradientBoosting': {
            'model': GradientBoostingClassifier(random_state=42),
            'params': {
                'n_estimators': [100, 200],
                'learning_rate': [0.01, 0.1],
                'max_depth': [3, 5]
            }
        }
    }

    # 7.2 Train and evaluate models with cross-validation
    best_models = {}
    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)

    print("\nTraining models with cross-validation and hyperparameter tuning:")
    for name, model_info in models.items():
        print(f"\nTraining {name}...")

        # Optional: Uncomment for full grid search (time-consuming)
        # grid_search = GridSearchCV(
        #     model_info['model'], model_info['params'], cv=cv, scoring='accuracy', n_jobs=-1
        # )
        # grid_search.fit(X_train, y_train)
        # best_model = grid_search.best_estimator_
        # print(f"Best parameters: {grid_search.best_params_}")
        # print(f"CV Score: {grid_search.best_score_:.4f}")

        # For now, use default parameters for speed
        model = model_info['model']

        # Extra check for XGBoost
        if name == 'XGBoost':
            print(f"XGBoost - Training with target values: {np.unique(y_train)}")
            # Explicitly set number of classes for XGBoost
            if hasattr(model, 'set_params'):
                model.set_params(num_class=len(np.unique(y)))

        model.fit(X_train, y_train)
        best_models[name] = model

        # Evaluate on test set
        y_pred = model.predict(X_test)
        acc = accuracy_score(y_test, y_pred)

        # Cross-validation score
        cv_scores = cross_val_score(model, X_scaled, y, cv=cv, scoring='accuracy')

        print(f"Model: {name}")
        print(f"Test Accuracy: {acc:.4f}")
        print(f"CV Accuracy: {cv_scores.mean():.4f} Â± {cv_scores.std():.4f}")
        print("Classification Report:")
        print(classification_report(y_test, y_pred))
        print("Confusion Matrix:")
        print(confusion_matrix(y_test, y_pred))

        # Probability predictions for ROC curve
        if hasattr(model, "predict_proba"):
            y_proba = model.predict_proba(X_test)

            # For multiclass, calculate ROC for each class
            n_classes = len(np.unique(y))
            if n_classes > 2:
                print(f"ROC-AUC scores for {n_classes} classes:")
                for i in range(n_classes):
                    fpr, tpr, _ = roc_curve(y_test == i, y_proba[:, i])
                    roc_auc = auc(fpr, tpr)
                    print(f"  Class {i}: ROC-AUC = {roc_auc:.4f}")
            else:
                # Binary classification
                fpr, tpr, _ = roc_curve(y_test, y_proba[:, 1])
                roc_auc = auc(fpr, tpr)
                print(f"ROC-AUC: {roc_auc:.4f}")

    # 8. Ensemble model (voting classifier)
    print("\nTraining Ensemble Model...")
    estimators = [(name, model) for name, model in best_models.items()]
    ensemble = VotingClassifier(estimators=estimators, voting='soft')
    ensemble.fit(X_train, y_train)

    ensemble_pred = ensemble.predict(X_test)
    ensemble_acc = accuracy_score(y_test, ensemble_pred)

    print("Ensemble Model Results:")
    print(f"Accuracy: {ensemble_acc:.4f}")
    print("Classification Report:")
    print(classification_report(y_test, ensemble_pred))
    print("Confusion Matrix:")
    print(confusion_matrix(y_test, ensemble_pred))

    # 9. Feature importance analysis
    print("\nFeature Importance Analysis:")

    # Get feature names from original dataset
    if isinstance(X_selected, np.ndarray):
        feature_names = [f"Feature_{i}" for i in range(X_selected.shape[1])]
    else:
        feature_names = X.columns[feat_selector.get_support()].tolist()

    # Random Forest feature importance
    rf_model = best_models['RandomForest']
    rf_importances = rf_model.feature_importances_

    # Sort and visualize top features
    rf_indices = np.argsort(rf_importances)[::-1]
    top_n = min(10, len(feature_names))

    plt.figure(figsize=(12, 8))
    plt.title('Feature Importances (Random Forest)')
    plt.barh(range(top_n), rf_importances[rf_indices][:top_n], align='center')
    plt.yticks(range(top_n), [feature_names[i] for i in rf_indices[:top_n]])
    plt.xlabel('Relative Importance')
    # plt.show()  # Uncomment to display

    # Save model performance to file
    model_results = {
        'Model': [],
        'Accuracy': [],
        'Precision': [],
        'Recall': [],
        'F1-Score': []
    }

    for name, model in best_models.items():
        y_pred = model.predict(X_test)
        report = classification_report(y_test, y_pred, output_dict=True)

        # For weighted average metrics
        model_results['Model'].append(name)
        model_results['Accuracy'].append(accuracy_score(y_test, y_pred))
        model_results['Precision'].append(report['weighted avg']['precision'])
        model_results['Recall'].append(report['weighted avg']['recall'])
        model_results['F1-Score'].append(report['weighted avg']['f1-score'])

    # Add ensemble model results
    model_results['Model'].append('Ensemble')
    report = classification_report(y_test, ensemble_pred, output_dict=True)
    model_results['Accuracy'].append(ensemble_acc)
    model_results['Precision'].append(report['weighted avg']['precision'])
    model_results['Recall'].append(report['weighted avg']['recall'])
    model_results['F1-Score'].append(report['weighted avg']['f1-score'])

    # Create results DataFrame
    results_df = pd.DataFrame(model_results)
    print("\nModel Performance Summary:")
    print(results_df)

    # Uncomment to save results
    # results_df.to_csv('liver_cirrhosis_model_results.csv', index=False)

else:
    print("Target column 'Stage' not found in dataset")